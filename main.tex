\documentclass[ngerman]{../LaTeX-Templates/Paper/paper}

\usepackage{nameref}
\usetikzlibrary{arrows,automata,positioning}


\title{Grundlagen der künstlichen Intelligenz}
\author{Simon König\\Skript und Zusammenfassung zur Vorlesung\\ von Marc \textsc{Toussaint}\\an der Universität Stuttgart}
\date{Stand \today}

\lfoot{Simon König \today}

\newcommand{\countset}[1]{\simpleset{1,\ldots,#1}}
\newcommand{\E}{\ensuremath{\operatorname{E}}}
\newcommand{\independent}{\ensuremath{\operatorname{Indep}}}
\newcommand{\enqo}[1]{\glqq #1\grqq\ }
\definecolor{green}{rgb}{0.2,0.8,0.2}
\lstset{ 
  backgroundcolor=\color{white},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=true,
  breaklines=true,
  captionpos=b,
  deletekeywords={},
  escapeinside={(*}{*)},
  mathescape=true,
  extendedchars=true,
  frame=lines,
  xleftmargin=10pt,
  keepspaces=true,
  keywordstyle=\bfseries\color{red!60!black},
  commentstyle=\itshape\color{gray!40!black},
  identifierstyle=\color{black},
  stringstyle=\color{green!40!black},      
  language=Java,
  morekeywords={*,...},
  numbers=left,
  numbersep=5pt,
  resetmargins=true,
  numberstyle=\tiny\color{gray!40!black},
  rulecolor=\color{black},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  aboveskip=\medskipamount,
  belowskip=-2\medskipamount,
  stepnumber=1,
  tabsize=2,
  title=\lstname,
  emph=[1]{%Klassen o.ä.
	TreePolicy,
	RolloutPolicy,
	Backup
  },
  emphstyle=[1]{\scshape\color{blue!60!black}},
  emph=[2]{
  then,
  end,
  function,
  },
  emphstyle=[2]{\bfseries\color{red!60!black}},
}




\begin{document}
\maketitle%
\tableofcontents

\section{Einführung}



\section{Banditen}
Wir wollen ein System analog zu einarmigen Banditen modellieren durch sogenannte \glqq Banditen\grqq. Wir sagen, es gibt $n$ mögliche Banditen zwischen denen man wählen kann, die zu treffende Entscheidung ist also einer der Banditen.

Jeder Bandit gibt einen Gewinn (\emph{reward}), der proportional zur Gewinnwahrscheinlichkeit des Banditen $y\sim P(y;\theta_i)$ ist. Hierbei sei der Parameter $\theta_i$ der Maschine unbekannt. Das Ziel ist es, den Gewinn über eine Menge von Zügen zu maximieren.

Dieses Entscheidungsmodell ist ein Archetyp für die verschiedensten Problemstellungen.

\begin{definition}[Banditenproblem]\label{Banditenproblem}
	Das \emph{Banditenproblem} besteht aus $n$ Banditen mit jeweils unbekannten Parametern $\theta_i$, die den Gewinn $y$ beeinflussen.
	Sei $a_t\in\countset n$ die Entscheidung für einen Banditen zum Zeitpunkt $t$. Das zugehörige Ergebnis ist $y_t\in\R$. 

	Eine Strategie (\emph{policy}) verwendet das Wissen über alle bisher getroffenen Entscheidungen um einen nächsten Banditen auszuwählen
	\begin{equation*}
		\pi:[(a_1,y_1),(a_2,y_2),\ldots,(a_{t-1},y_{t-1})]\mapsto a_t.
	\end{equation*}
\end{definition}
Eine Problemstellung kann nun auf verschiedene Weisen definiert werden, so gibt es zwei grundlegende \enqo{Ziel-Typen}. Man kann versuchen während des gesamten Zeitraums einen maximalen Gewinn zu erzielen. Das heißt Ziel ist es, ein $\pi$ zu finden, sodass der Gewinn insgesamt maximiert wird
\begin{equation}
	\max\left\langle\sum_{t=1}^Ty_t\right\rangle.\label{policy_exploit}
\end{equation}
Dieses Verhalten würde man als \emph{exploitation} bezeichnen.
Zum Beispiel könnte immer $a_t$ so gewählt werden, dass das maximale $y_t$ zu erwarten ist.

Auch möglich ist, zu fordern, dass nur die letzte Entscheidung die bestmögliche ist, das heißt
\begin{equation}
	\max\left\langle y_T\right\rangle,\label{policy_explore}
\end{equation}
was bedeutet, dass in den Schritten vor der letzten Entscheidung ohne Rücksicht auf die Rewards einer Entscheidung gelernt werden kann. Man setzt hier also den Fokus darauf zu \emph{explorieren} und alle Möglichkeiten auszuprobieren.
Man versucht mit der \emph{policy} aus \autoref{policy_explore} möglichst viel Wissen zu sammeln. Es wird auf exploration optimiert und versucht so die Unsicherheit über die Wahrscheinlichkeitsverteilung der Banditen zu minimieren.


Wissen über das System kann auf zwei Arten und Weisen dargestellt werden. Zum Einen durch den Verlauf der bisherigen Interaktion zwischen Agent und Banditen
\begin{equation*}
	h_t=[(a_1,y_1),(a_2,y_2),\ldots,(a_{t-1},y_{t-1})].
\end{equation*}
Aber auch eine Darstellung als Wahrscheinlichkeitsverteilung ist möglich. Der sogenannte \emph{belief state} ist
\begin{equation*}
	b_t(\theta)=P(\theta|h_t),
\end{equation*}
wobei $\theta=(\theta_1,\ldots,\theta_n)$ der Vektor der unbekannten Gewinnwahrscheinlichkeiten der einzelnen Banditen ist.




Wir betrachten im Folgenden einige Möglichkeiten Banditen nach einer Problemstellung zu bewerten und so Entscheidungen zu treffen.
\subsection{Upper Confidence Bound (UCB)}
Mit der \emph{upper confidence bound}-Methode versucht man einen guten Mittelweg zwischen exploitation und exploration zu gehen. 

Zunächst wählt man jeden Banditen (mindestens) einmal. Gute Entscheidungen kann man natürlich nur dann treffen, wenn zu jedem Banditen überhaupt Informationen bekannt sind. 

Das Ziel ist es, den Banditen zu wählen, der zum einen den höchsten Gewinn verspricht aber bei dem man zum anderen noch neue Informationen lernen kann.
Betrachtet man den Gewinn jedes Banditen als zufällig verteilte Variable, werden wir den Mittelwert schätzen und zukünftig versuchen die Unsicherheit über diese Schätzung zu verringern.

Sei also $n$ die Anzahl der Züge insgesamt, $n_i$ die Anzahl der Spielzüge und $\Delta_i$ die Summe der Gewinne, jeweils am Banditen $i$.

Wir schätzen den Mittelwert der Gewinne bisher als
\begin{equation*}
	\hat y_i=\frac{\Delta_i}{n_i}.
\end{equation*}
Für eine Konstante $\beta$ (meist $\beta=1$) wählen wir dann den Banditen $i$, der
\begin{equation*}
	\hat y_i+\beta \sqrt{\frac{2\ln n}{n_i}}
\end{equation*}
maximiert. 
Mit dieser Entscheidung bilden wir ein Konfidenzintervall um den tatsächlichen Mittelwert $y_i$, das heißt mit einer (relativ hohen) Wahrscheinlichkeit gilt
\begin{equation*}
	\hat y_i-\sigma_i<y_i<\hat y_i+\sigma_i.
\end{equation*}
Der Algorithmus wählt jeweils die obere Schranke als Vergleichswert der Banditen. Damit wird sowohl der geschätzte Gewinn als auch die Unsicherheit der Schätzung mit einbezogen.


\subsection{Monte Carlo Tree Search (MCTS)}
Die Monte Carlo-Simulation versucht mit einer großen Zahl an zufälligen Stichproben eine Verteilung $x_i\sim P(x)$ zu approximieren. Mit diesen Stichproben kann man eine Verteilung abschätzen. Zum Beispiel mit 
\begin{equation*}
	\langle f\rangle=\int_xP(x)f(x)\intd x\approx \frac1N\sum_{i=1}^Nf(x_i),
\end{equation*}
ein Integral.

Bei der \emph{Monte Carlo-Baumsuche (MCTS)} versucht man den erwarten Reward abhängig von einer gewählten Aktion $a$ zu schätzen. Genauer gesagt wird die $Q$-Funktion 
\begin{equation*}
	Q(s_0,a)=\E\set{\Delta}{s_0,a}
\end{equation*}
geschätzt, wobei der erwartete Reward durch ein Monte Carlo-Verfahren bestimmt wird. Das heißt man simuliert randomisiert viele \glqq Zukünfte\grqq\ und schätzt so den Reward abhängig von $a$. Das Simulieren einer Zukunft wird auch Rollout genannt.

Ein generisches MCTS-Verfahren sieht dann wie folgt aus.
\begin{lstlisting}
start tree V = {(*$v_0$*)}
while within budget do
	(*$v_l$*) := TreePolicy(V)
	append (*$v_l$*) to V
	(*$\Delta$*) := RolloutPolicy(V)
	Backup((*$v_l, \Delta$*))
end while
return best child of (*$v_0$*)
\end{lstlisting}

\begin{itemize}
	\item Mit der \textsc{TreePolicy}-Methode wird entschieden welcher Blattknoten expandiert wird. Der Rückgabewert ist also der vielversprechendste Knoten, abhängig von der Bewertungsfunktion.
	\item Die \textsc{RolloutPolicy} simuliert einen Rollout. Oft wird dies zufällig berechnet, mindestens jedoch randomisiert.
	\item \textsc{Backup} reicht den Reward vom Blattknoten zurück bis zur Wurzel durch. So kann im nächsten TreePolicy-Schritt von der Wurzel aus der beste Knoten ausgewählt werden. 
\end{itemize}
Man spricht von \emph{Flat Monte Carlo}, wenn die ausgerollten Zukünfte nicht gespeichert werden. Der Baum kann damit vergleichsweise flach gehalten werden. Entscheidend ist, dass das Baumwachstum - und damit der Speicheraufwand - mit diesem Verfahren auf vielversprechende Pfade fokussiert wird.

\paragraph{Upper Confidence Tree (UCT)}
Man kann auf das MCTS-Verfahren das bereits bekannte {\itshape Upper Confidence Bound}-System anwenden. Es handelt sich dabei dann um {\itshape Upper Confidence Tree (UCT)}.
\begin{itemize}
	\item Man wählt mithilfe von UCB den nächsten Knoten. Das heißt die \textsc{TreePolicy} wählt
	\begin{equation*}
		\underset{v'\in\partial(v)}\argmax\frac{Q(v')}{n(v')}+\beta\sqrt{\frac{2\ln n(v)}{n(v')}}
	\end{equation*}
	zum Expandieren.
	\item Die \textsc{Backup}-Methode aktualisiert alle Eltern $v$ von $v_l$ mit
	\begin{align*}
		n(v)&\leftarrow n(v)+1&&\text{Anzahl Rollouts}\\
		Q(v)&\leftarrow Q(v)+\Delta&&\text{Summe der Rewards.}
	\end{align*}
\end{itemize}


\subsection{Game Playing}
Die nachfolgenden Strategien sind für Spiele entworfen, in denen zwei Spieler gegeneinader antreten.


04/28ff
\paragraph{Minimax}
Die Idee des Minimax ist, den Pfad auszuwählen, der bei optimalem Opponenten noch den größten Gewinn verspricht.

Fassen wir den Gewinn des Spiels als eine Zahl (Reward) auf, lässt sich dieses Prinzip als rundenweises Minimieren bzw. Maximieren auffassen.
Der Spieler möchte den Reward maximieren, während das Ziel des Gegners ist, den Reward des Spielers zu minimieren. 

Bei einer Entscheidung des Spielers wird also der Pfad gewählt, der maximalen Gewinn verspricht. Simulieren wir nun Züge des Gegners, gehen wir von optimalem Handeln aus und lassen den Gegner den Reward minimieren. 

\begin{equation*}
	\underset a\argmax \mathrm{MinValue}()
\end{equation*}






Minimax findet einen optimalen Pfad, wenn der Baum endlich ist.
Allerdings wird ein optimaler Pfad nur dann gefunden wenn die Annahme, dass der Gegner optimal spielt stimmt. Ansonsten ist das Verhalten unklar und möglicherweise sehr schlecht.

Sei $b$ der branching factor, also die Zahl der möglichen Entscheidungen pro Zug (oder zumindest ein gutes Mittel) und $m$ die Zahl der Züge. Dann ist der Zeitaufwand die Größe des Baums, also $\mathcal O(b^m)$. Der Platzaufwand ist $\mathcal O(b*m)$, wie für eine Tiefensuche üblich.

\paragraph{$\alpha$-$\beta$ pruning}
Mit dem \emph{$\alpha$-$\beta$ pruning} können wir eine Verbesserung des Minimax-Verfahrens schaffen. 
Für viele Teilbäume kann schon sehr früh entschieden werden, dass sie keine guten Spielausgänge versprechen. Durch pruning des Baums kann der Zeitaufwand deutlich verbessert werden, dabei bleibt das Ergebnis trotzdem unverändert.



\paragraph{UCT for games}







\input{MDPs.tex}
















\section{Reinforcement Learning}
Bisher sind wir davon ausgegangen, die Welt zu kennen. Genauer heißt das, dass $P(s'|s,a)$ und $R(s,a)$ bekannt waren. Damit konnten wir mittels Value- bzw. Q-Iteration (also mit dynamischem Programmieren) optimale Policies bestimmen.

Wenn diese Werte bzw. Verteilungen nicht bekannt sind, befindet man sich im Themengebiet des reinforcement Lernens.
Ein mit der Welt interagierender Agent sammelt den Datensatz
\begin{equation*}
	D=\simpleset{(s_t,a_t,r_t,s_{t+1})}_{t=1}^T
\end{equation*}
und versucht daraus zu lernen.

Man muss hierbei unterscheiden, ob ein Modell der Welt bekannt ist oder nicht. Das heißt, ob ein Agent alle möglichen Zustände und Aktionen erfassen kann oder nicht. Je nachdem ob man modellbasiert oder modellfrei lernt sind die Lernansätze unterschiedlich.
\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
                semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,rectangle,draw=none,text=black,align=center]

	  \node[state] 		   (exp)                    	{\bf Erfahrung\\$\simpleset{s_t,a_t,s_{t+1},r_t}$};
	  \node[state]         (beh) [below=5cm of exp] 		{\bf Verhalten\\$\pi$};
	  
	  \begin{scope}[transparency group, opacity=0.5]
	  	\path (exp) edge[bend right=90,red,line width=3pt,-stealth] node[rotate=90,anchor=center,above]{modellbasiert} (beh);
	  	\path (exp) edge[bend left=90,green,line width=3pt,-stealth] node[rotate=-90,anchor=center,above]{modellfrei}(beh);
	  \end{scope}
	  


	  \node[state]         (kno) [below left=2cm and -0.6cm of exp] 		{\bf Wissen\\$P(s_{t+1}|a_t,s_t)$};
	  \node[state]         (val) [below right=2cm and 0cm of exp] 		{\bf Value\\$V,Q$};
	  

	  \path (exp) edge node[sloped, anchor=center, above]{\small Modell lernen} (kno) edge node[sloped, anchor=center, above]{\small Value Learning} (val)
	  (kno) edge node[sloped, anchor=center, below]{\small Planung} (beh)
	  (val) edge node[sloped, anchor=center, below]{\small Aktionswahl} (beh);
	  
	\end{tikzpicture}
\end{center}\noindent
Wir betrachten die genauen Unterschiede in den nachfolgenden Abschnitten.

\subsection{Modellfreies Reinforcement Learning}
Im modellfreien Reinforcement-Lernen sind die Übergangsverteilungen sowie die Rewards der Zustände unbekannt, es müssen nicht einmal alle Zustände bekannt werden. 
Ziel ist es zu lernen, den Value eines Zustands vorherzusagen. Das heißt die Algorithmen werden $V(s)$ bzw. $Q(s,a)$ schätzen.

\paragraph{On-Policy vs. Off-Policy Learning}
Es werden zwei Arten unterschieden. Ein On-Policy Lernverfahren lernt die Bewertung des Zustands und der Aktionen die von der Policy tatsächlich ausgeführt werden. Das heißt während der Agent $\pi$ ausführt wird auch $Q^\pi$ gelernt.

Ein Off-Policy-Verfahren hingegen lernt die optimale $Q$-Funktion $Q^\ast$ während eine \emph{beliebige} Policy $\pi$ ausgeführt wird. Die Policy wird lediglich zum Sammeln von Daten verwendet. Die Policy, nach der der Agent handelt ist nicht die gleiche wie die gelernte.


\subsubsection{Q-Learning}
Betrachten wir die Bellman-Optimalitätsgleichung für die $Q$-Funktion
\begin{equation*}
	Q^\ast(s,a)=R(s,a)+\gamma\sum_{s'}P(s'\,|\, s,a)\max_{a'}Q^\ast(s',a').
\end{equation*}
Wir möchten wieder basierend auf dieser Gleichung einen Algorithmus entwickeln der genauso wie die Q-Iteration ein $Q^\ast$ approximiert.
Doch ohne Modell der Welt kennen wir den zweiten Teil der Formel nicht, weder die möglichen Aktionen $a'$ noch die möglichen Zustände $s'$ sind bekannt.

Stattdessen lernen wir aus einzelnen Erfahrungen $(s,a,r,s')$ um so die $Q$-Funktion zu schätzen. Für jede Erfahrung berechnen wir basierend auf der vorherigen Q-Funktion die neue Approximation
\begin{align*}
	Q_{\text{new}}(s,a)&=(1-\alpha)Q_{\text{old}}(s,a)\\
	&\quad+\alpha[r+\gamma\max_{a'}Q_{\text{old}}(s',a')],
	\intertext{was wir weiter umformen zu}
	Q_{\text{new}}(s,a)&=Q_{\text{old}}(s,a)\\
	&\quad+\alpha\underbrace{[r+\gamma\max_{a'}Q_{\text{old}}(s',a')-Q_{\text{old}}(s,a)]}_{\text{TD error}}.
\end{align*}
So wird mit jedem Schritt der sogenannte \emph{temporal difference error} berechnet und als Korrekturfaktor addiert.
Ist der Reward $r$ größer als der erwartete Reward, das heißt
\begin{equation*}
	r>Q_{\text{old}}(s,a)-\gamma \max_{a'}Q_{\text{old}}(s',a'),
\end{equation*}
dann wird $Q_{\text{new}}(s,a)$ erhöht. Analog wird der Wert verringert, wenn $r$ kleiner als die Erwartung ist.
Dieses Verfahren wird \emph{Q-Learning} genannt, es ist ein Off-Policy-Lernalgorithmus.

Besonders bemerkenswert ist, dass eine mit Q-Learning gelernte Q-Funktion beweisbar gegen $Q^\ast$ konvergiert. Für den Beweis wurde lediglich die \emph{mixing property} angenommen. Das heißt, dass es von jedem Zustand aus eine Wahrscheinlichkeit $>0$ gibt in jeden anderen Zustand zu gelangen.

Ein Pseudocode des Q-Learning-Algorithmus sieht dann wie folgt aus.
\begin{lstlisting}
(*$Q(s,a)=0$*)
while unhappy
	initialize start state (*$s$*)
	while in episode
		Choose action (*$a\approx_\epsilon\underset a\argmax Q(s,a)$*)
		Take action (*$a$*), observe (*$r,s'$*)
		(*$Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma\max_{a'}Q(s',a')-Q(s,a)]$*)
		(*$s\leftarrow s'$*)
	end
end\end{lstlisting}
Hierbei wurde die sogenannte \emph{epsilon greedy}-Strategie ($\approx_\epsilon$) verwendet. Das bedeutet lediglich, dass mit Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion gewählt wird und nur mit Wahrscheinlichkeit $1-\epsilon$ tatsächlich das gewünschte $\argmax$. In Formel also
\begin{gather*}
	a\approx_\epsilon \underset a\argmax Q(s,a)\Longleftrightarrow\\ a=\begin{cases}
		\text{random},&\text{Wahrscheinlichkeit }\epsilon\\
		\underset a\argmax Q(s,a),&\text{sonst.}
	\end{cases}
\end{gather*}
Damit wird sichergestellt, dass der Algorithmus genügend exploriert und neue Daten sammelt, also eben nicht immer die optimale Aktion ausführt. Somit werden zum Beispiel lokale Maxima vermieden.

Desweiteren wird der Algorithmus in sogenannten Episoden ausgeführt, nach denen jedes mal der Startzustand wiederhergestellt wird bevor der Agent weiter agieren kann. Oft ist die Anzahl der auszuführenden Schritte in einer Episode festgelegt. Episoden stellen sicher, dass der Agent nicht in non-return areas gefangen bleibt und Lernen für immer verhindert wird (Klippe heruntergefallen, Objekt mit dem interagiert wird ist kaputt,\ldots).

\subsubsection{Varianten des Q-Learning}
Es gibt viele Varianten des Basis-Q-Learning. Sie
\paragraph{Eligibility Traces}
Bisher sind die Lernverfahren sehr langsam, da ein erzielter Reward immer nur einen Schritt \enqo{in die Vergangenheit} weitergereicht wird.
Aber alle Zustände entlang eines Pfads, der zu einem Erfolg führt, sollten aktualisiert werden. Erhält einen Reward, ist nicht nur die letzte getroffene Entscheidung die entscheidende Aktion, sondern der gesamte gegangene Pfad ist wichtig.
\emph{Eligibility Traces} bewerten (mit Discounting und Skalierungsfaktor) den gesamten Pfad so wird aus der gesamten Vergangenheit bis zum Reward gelernt. Die $Q$-Tabelle wird viel schneller gefüllt und damit schneller gelernt.



\begin{itemize}
	\item Experience Replay
\end{itemize}
\paragraph{TD}
Der \emph{Temporal Difference}-Lernalgorithmus lernt aus der Erfahrung $(s,r,s')$, indem die Valuefunktion im Zustand $s$ durch
\begin{equation*}
	V(s)\leftarrow V(s)+\alpha [r+\gamma V_{\text{old}}(s')-V_{\text{old}}(s)]
\end{equation*}
aktualisiert wird.

Mit Verwendung von einer längeren Historie kann man ganze Erfahrungssequenzen mit einbeziehen, zum Beispiel $(s_0,r_0,r_1,r_2,s_3)$ und damit
\begin{equation*}
	V(s_0)\leftarrow V(s_0)+\alpha [r_0+\gamma r_1+\gamma^2r_2+\gamma^3 V_{\text{old}}(s_3)-V_{\text{old}}(s_0)]
\end{equation*}
berechnen. Dies nennt man dann \emph{Temporal Credit Assignment}.

Bei den beiden Verfahren wurde bisher immer nur ein Zustand aktualisiert, wir betrachten nun den $\mathrm{TD}(\lambda)$-Algorithmus, der Eligibility auf elegante Art und Weise unterbringt und damit mit einer Erfahrungssequenz mehrere Zustände aktualisieren kann.

Die Eligibility-Funktion wird immer beim Besuchen des Zustands $s_t$ aktualisiert
\begin{equation*}
	e(s_t)\leftarrow e(s_t)+1.
\end{equation*}
Die Funktion \enqo{merkt sich}, dass der Zustand vor kurzem besucht wurde. Unter Verwendung dieser Funktion wird dann die Valuefunktion aktualisiert
\begin{equation*}
	\forall s:V_{\text{new}}(s)= V_{\text{old}}(s)+\alpha e(s)*[r_t+\gamma V_{\text{old}}(s_{t+1})-V_{\text{old}}(s_t)].
\end{equation*}
Wichtig ist hierbei, dass alle Zustände aktualisiert werden.
Zustände die nicht oder lange nicht mehr besucht wurden haben dabei einen kleinen Eligibility-Wert, da man in jedem Schritt das Discounting
\begin{equation*}
	\forall s:e(s)\leftrightarrow \gamma\lambda e(s)
\end{equation*}
anwendet.

\paragraph{SARSA}
SARSA verwendet die Erfahrung $(s,a,r,s',a')$ (daher auch der Name) um $Q^\pi$-Funktion zu schätzen.
\begin{equation*}
	\forall s:Q(s,a)\leftarrow Q(s,a)+\alpha e(s,a)[r+\gamma Q_{\text{old}}(s',a')-Q_{\text{old}}(s,a)]
\end{equation*}

\paragraph{$Q(\lambda)$}
\begin{equation*}
	\forall s:Q(s,a)\leftarrow Q(s,a)+\alpha e(s,a)[r+\gamma \max_{a'} Q_{\text{old}}(s',a')-Q_{\text{old}}(s,a)]
\end{equation*}





\subsection{Modellbasiertes Reinforcement Learning}
06/23ff




Im modellbasierten Reinforcement Learning 

Der Agent wird dann versuchen
\begin{itemize}
	\item den nächsten Zustand vorherzusagen, genauer $P(s'|s,a)$ und
	\item den zugehörigen Reward, also $P(r|s,a)$ zu schätzen.
\end{itemize}







todo


\begin{itemize}
	\item Belief State
	\item Lernen in MDPs, modellbasiert/modellfrei
	\item Diskussion Grenzen von modellfreiem Lernen 
	\item R-MAX
\end{itemize}




\subsection{Q-learning} Folie 11/50



Aus den neuen Erfahrungen die Zustände und Aktionen zu bewerten ist Reinforcement Learning

Schätzen und mit Lernrate $\alpha$ langsam annähern. Ohne Lernrate ist der Teil nach $\alpha$ zu stochastisch verrauscht. Neue Schätzung mit Low-Pass-Filter versehen. Lernen aus einer einzigen Erfahrung.

R(s,a): Erwartungswert der sofortigen Belohnung wenn ich Aktion a in Zustand s ausführe

Q-Funktion: Wie viel 

V-Funktion: $V^\pi(s)$ Return den ich erwarte wenn ich Policy $\pi$ benutze. $V^\ast$ optimales Verhalten









\input{CSPs.tex}










\section{Graphische Modelle von probabilistischen Systemen}











Unabhängigkeit von Zufallsvariablen sollte klar sein, wir haben wir diese Eigenschaft streng genommen sogar bereits benutzt, der Vollständigkeit halber definieren wir sie hier noch einmal.
\begin{definition}[Unabhängigkeit]
	Zwei Zufallsvariablen $X,Y$ heißen \emph{unabhängig}, wenn
	\begin{equation*}
		\independent(X,Y)\Longleftrightarrow P(X,Y)=P(X)*P(Y)
	\end{equation*}
	gilt. Analog sind sie \emph{konditional unabhängig}, wenn
	\begin{equation*}
		\independent(X,Y|Z)\Longleftrightarrow P(X,Y|Z)=P(X|Z)*P(Y|Z).
	\end{equation*}
\end{definition}


\subsection{Bayesnetze}
09/*ff



\begin{definition}[Bayesnetz]
	Ein \emph{Bayesnetz} ist ein gerichteter azyklischer Graph, bei dem jeder Knoten eine Zufallsvariable repräsentiert. Für jeden Knoten $X_i$ gibt es eine konditionale Wahrscheinlichkeitsverteilung für dessen Wert
	\begin{equation*}
		P(X_i\,|\,\mathrm{Parents}(X_i)).
	\end{equation*}
	Üblicherweise stellt man beobachtete Variablen d.h. Variablen, deren Werte bekannt sind, mit grauer Schattierung dar.
\end{definition}
Die gemeinsame Verteilung kann man durch
\begin{equation*}
	P(X_{1:n})=\prod_{i=1}^nP(X_i\,|\,\mathrm{Parents}(X_i))
\end{equation*}
faktorisieren. Wobei $\mathrm{Parents}(X_i)$ die Vorgänger von $X_i$ sind, d.h. ein Knoten $v\in\mathrm{Parents}(X_i)$ genau dann, wenn eine Kante $(v,X_i)$ im Bayesnetz existiert.

Um Unabhängigkeit von Variablen in Bayesnetzen zu entscheiden gibt es grundsätzlich drei Fälle, die eine Hilfe sein können.
\begin{enumerate}
	\item Head to Head:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [below right of=X] 		{$Z$};
		  \node[state]         (Y) [above right of=Z] 		{$Y$};

		  \path (X) edge (Z)
		  (Y) edge (Z);
		\end{tikzpicture}
	\end{center}
	Es gilt $\independent(X,Y)$ und $\neg\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(X)P(Y)P(Z|X,Y)\\
		P(X,Y)&=P(X)P(Y)\sum_ZP(Z|X,Y)\\
		&=P(X)P(Y).
	\end{align*}

\item Tail to Tail:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [above right of=X] 		{$Z$};
		  \node[state]         (Y) [below right of=Z] 		{$Y$};

		  \path (Z) edge (X) edge (Y);
		\end{tikzpicture}
	\end{center}
	Es gilt $\neg\independent(X,Y)$ und $\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(Z)P(X|Z)P(Y|Z)\\
		P(X,Y|Z)&=\frac{P(X,Y,Z)}{P(Z)}\\
		&=P(X|Z)P(Y|Z).
	\end{align*}

\item Head to Tail:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [above right of=X] 		{$Z$};
		  \node[state]         (Y) [below right of=Z] 		{$Y$};

		  \path (X) edge (Z) 
		  (Z) edge (Y);
		\end{tikzpicture}
	\end{center}
	Es gilt $\neg\independent(X,Y)$ und $\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(X)P(Z|X)P(Y|Z)\\
		P(X,Y|Z)&=\frac{P(X,Y,Z)}{P(Z)}=\frac{P(X,Z)P(Y|Z)}{P(Z)}\\
		&=P(X|Z)P(Y|Z).
	\end{align*}

\end{enumerate}



\subsection{Inferenz in graphischen Modellen}
In Bayesnetzen können wir genauso Inferenz betreiben und aus Informationen der beobachteten Variablen oder a priori-Informationen Aussagen über andere Variablen treffen.

\subsubsection{Variablenelimination}
\begin{enumerate}
	\item Variablenelimination
	\item Faktorgraphen
	\item Message Passing
	\item Loopy belief propagation
	\item Sampling Methods?
\end{enumerate}

\subsection{Decision Making}
\subsection{Learning}
\subsection{Structure Learning}

\section{Dynamische Modelle}
\subsection{Markovprozesse und -ketten}
Markovprozess erster Ordnung
\vspace{1em}
\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.3cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

	  \node[state] 		   (x-2)                    	{\scriptsize$X_{t-2}$};
	  \node[state]         (x-1) [right of=x-2] 		{\scriptsize$X_{t-1}$};
	  \node[state]         (x) [right of=x-1] 		{\scriptsize$X_{t}$};

	  \node[state]         (x1) [right of=x] 		{\scriptsize$X_{t+1}$};
	  \node[state]         (x2) [right of=x1]       {\scriptsize$X_{t+2}$};

	  \node[text width=0pt]			(start) [left=0.5cm of x-2] {};
	  \node[text width=0pt]			(end) [right=0.5cm of x2] {};

	  \path (start) edge (x-2)
	  (x-2) edge (x-1)
	  (x-1) edge (x)
	  (x) edge (x1)
	  (x1) edge (x2)
	  (x2) edge (end);
	\end{tikzpicture}
\end{center}
\vspace{1em}
Markovprozess zweiter Ordnung
\vspace{1em}
\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.3cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

	  \node[state] 		   (x-2)                    	{\scriptsize$X_{t-2}$};
	  \node[state]         (x-1) [right of=x-2] 		{\scriptsize$X_{t-1}$};
	  \node[state]         (x) [right of=x-1] 		{\scriptsize$X_{t}$};

	  \node[state]         (x1) [right of=x] 		{\scriptsize$X_{t+1}$};
	  \node[state]         (x2) [right of=x1]       {\scriptsize$X_{t+2}$};

	  \node[text width=0pt]			(start) [left=0.5cm of x-2] {};
	  \node[text width=0pt]			(end) [right=0.5cm of x2] {};

	  \path (start) edge (x-2) edge[in=135, out=45] (x-1)
	  (x-2) edge (x-1) edge[in=135, out=45] (x)
	  (x-1) edge (x) edge[in=135, out=45] (x1)
	  (x) edge (x1) edge[in=135, out=45] (x2)
	  (x1) edge (x2) edge[in=135, out=45] (end)
	  (x2) edge (end);
	\end{tikzpicture}
\end{center}
\vspace{1em}




\begin{itemize}
	\item Markov Prozesse
	\item Hidden Markov Models
\end{itemize}


\section{Neuronale Netze}
\subsection{Graphische Repräsentationen}


\section{Erklärbarkeit von Entscheidungen}







\section{unassigned}
\begin{itemize}
	\item Belief Space Planning for Bandits 07/22ff
\end{itemize}
\end{document}
