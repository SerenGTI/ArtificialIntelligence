\documentclass[ngerman]{../LaTeX-Templates/Paper/paper}

\usepackage{nameref}
\usetikzlibrary{arrows,automata,positioning}


\title{Grundlagen der künstlichen Intelligenz}
\author{Simon König\\Skript und Zusammenfassung zur Vorlesung\\ von Marc \textsc{Toussaint}\\an der Universität Stuttgart}
\date{Stand \today}

\lfoot{Simon König \today}

\newcommand{\countset}[1]{\simpleset{1,\ldots,#1}}
\newcommand{\E}{\ensuremath{\operatorname{E}}}
\newcommand{\independent}{\ensuremath{\operatorname{Indep}}}
\newcommand{\enqo}[1]{\glqq #1\grqq\ }
\definecolor{green}{rgb}{0.2,0.8,0.2}
\definecolor{observed}{rgb}{0.75,0.75,0.75}
\lstset{ 
  backgroundcolor=\color{white},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=true,
  breaklines=true,
  captionpos=b,
  deletekeywords={},
  escapeinside={(*}{*)},
  mathescape=true,
  extendedchars=true,
  frame=lines,
  xleftmargin=10pt,
  keepspaces=true,
  keywordstyle=\bfseries\color{red!60!black},
  commentstyle=\itshape\color{gray!40!black},
  identifierstyle=\color{black},
  stringstyle=\color{green!40!black},      
  language=Java,
  morekeywords={*,...},
  numbers=left,
  numbersep=5pt,
  resetmargins=true,
  numberstyle=\tiny\color{gray!40!black},
  rulecolor=\color{black},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  aboveskip=\medskipamount,
  belowskip=-2\medskipamount,
  stepnumber=1,
  tabsize=2,
  title=\lstname,
  emph=[1]{%Klassen o.ä.
	TreePolicy,
	RolloutPolicy,
	Backup
  },
  emphstyle=[1]{\scshape\color{blue!60!black}},
  emph=[2]{
  then,
  end,
  function,
  },
  emphstyle=[2]{\bfseries\color{red!60!black}},
}




\begin{document}
\maketitle%
\tableofcontents

\section{Einführung}



\section{Banditen}
Wir wollen ein System analog zu einarmigen Banditen modellieren durch sogenannte \glqq Banditen\grqq. Wir sagen, es gibt $n$ mögliche Banditen zwischen denen man wählen kann, die zu treffende Entscheidung ist also einer der Banditen.

Jeder Bandit gibt einen Gewinn (\emph{reward}), der proportional zur Gewinnwahrscheinlichkeit des Banditen $y\sim P(y;\theta_i)$ ist. Hierbei sei der Parameter $\theta_i$ der Maschine unbekannt. Das Ziel ist es, den Gewinn über eine Menge von Zügen zu maximieren.

Dieses Entscheidungsmodell ist ein Archetyp für die verschiedensten Problemstellungen.

\begin{definition}[Banditenproblem]\label{Banditenproblem}
	Das \emph{Banditenproblem} besteht aus $n$ Banditen mit jeweils unbekannten Parametern $\theta_i$, die den Gewinn $y$ beeinflussen.
	Sei $a_t\in\countset n$ die Entscheidung für einen Banditen zum Zeitpunkt $t$. Das zugehörige Ergebnis ist $y_t\in\R$. 

	Eine Strategie (\emph{policy}) verwendet das Wissen über alle bisher getroffenen Entscheidungen um einen nächsten Banditen auszuwählen
	\begin{equation*}
		\pi:[(a_1,y_1),(a_2,y_2),\ldots,(a_{t-1},y_{t-1})]\mapsto a_t.
	\end{equation*}
\end{definition}
Eine Problemstellung kann nun auf verschiedene Weisen definiert werden, so gibt es zwei grundlegende \enqo{Ziel-Typen}. Man kann versuchen während des gesamten Zeitraums einen maximalen Gewinn zu erzielen. Das heißt Ziel ist es, ein $\pi$ zu finden, sodass der Gewinn insgesamt maximiert wird
\begin{equation}
	\max\left\langle\sum_{t=1}^Ty_t\right\rangle.\label{policy_exploit}
\end{equation}
Dieses Verhalten würde man als \emph{exploitation} bezeichnen.
Zum Beispiel könnte immer $a_t$ so gewählt werden, dass das maximale $y_t$ zu erwarten ist.

Auch möglich ist, zu fordern, dass nur die letzte Entscheidung die bestmögliche ist, das heißt
\begin{equation}
	\max\left\langle y_T\right\rangle,\label{policy_explore}
\end{equation}
was bedeutet, dass in den Schritten vor der letzten Entscheidung ohne Rücksicht auf die Rewards einer Entscheidung gelernt werden kann. Man setzt hier also den Fokus darauf zu \emph{explorieren} und alle Möglichkeiten auszuprobieren.
Man versucht mit der \emph{policy} aus \autoref{policy_explore} möglichst viel Wissen zu sammeln. Es wird auf exploration optimiert und versucht so die Unsicherheit über die Wahrscheinlichkeitsverteilung der Banditen zu minimieren.

Bisher haben wir Wissen dargestellt als die gesamte Historie der Entscheidungen und damit verbundenen Rewards
\begin{equation*}
	h_t=[(a_1,y_1),(a_2,y_2),\ldots,(a_{t-1},y_{t-1})].
\end{equation*}
Aber auch eine Darstellung als Wahrscheinlichkeitsverteilung ist möglich. Der  \emph{belief state} ist
\begin{equation*}
	b_t(\theta)=P(\theta|h_t),
\end{equation*}
wobei $\theta=(\theta_1,\ldots,\theta_n)$ der Vektor der unbekannten Gewinnwahrscheinlichkeiten der einzelnen Banditen ist.

Wir betrachten im Folgenden einige Möglichkeiten Banditen nach einer Problemstellung zu bewerten und so Entscheidungen zu treffen.
\subsection{Upper Confidence Bound (UCB)}
Mit der \emph{upper confidence bound}-Methode versucht man einen guten Mittelweg zwischen exploitation und exploration zu gehen. 

Zunächst wählt man jeden Banditen (mindestens) einmal. Gute Entscheidungen kann man natürlich nur dann treffen, wenn zu jedem Banditen überhaupt Informationen bekannt sind. 

Das Ziel ist es, den Banditen zu wählen, der zum einen den höchsten Gewinn verspricht aber bei dem man zum anderen noch neue Informationen lernen kann.
Betrachtet man den Gewinn jedes Banditen als zufällig verteilte Variable, werden wir den Mittelwert schätzen und zukünftig versuchen die Unsicherheit über diese Schätzung zu verringern.

Sei also $n$ die Anzahl der Züge insgesamt, $n_i$ die Anzahl der Spielzüge und $\Delta_i$ die Summe der Gewinne, jeweils am Banditen $i$.

Wir schätzen den Mittelwert der Gewinne bisher als
\begin{equation*}
	\hat y_i=\frac{\Delta_i}{n_i}.
\end{equation*}
Für eine Konstante $\beta$ (meist $\beta=1$) wählen wir dann den Banditen $i$, der
\begin{equation*}
	\hat y_i+\beta \sqrt{\frac{2\ln n}{n_i}}
\end{equation*}
maximiert. 
Mit dieser Entscheidung bilden wir ein Konfidenzintervall um den tatsächlichen Mittelwert $y_i$, das heißt mit einer (relativ hohen) Wahrscheinlichkeit gilt
\begin{equation*}
	\hat y_i-\sigma_i<y_i<\hat y_i+\sigma_i.
\end{equation*}
Der Algorithmus wählt jeweils die obere Schranke als Vergleichswert der Banditen. Damit wird sowohl der geschätzte Gewinn als auch die Unsicherheit der Schätzung mit einbezogen.


\subsection{Monte Carlo Tree Search (MCTS)}
Die Monte Carlo-Simulation versucht mit einer großen Zahl an zufälligen Stichproben eine Verteilung $x_i\sim P(x)$ zu approximieren. Mit diesen Stichproben kann man eine Verteilung abschätzen. Zum Beispiel mit 
\begin{equation*}
	\langle f\rangle=\int_xP(x)f(x)\intd x\approx \frac1N\sum_{i=1}^Nf(x_i),
\end{equation*}
ein Integral.

Bei der \emph{Monte Carlo-Baumsuche (MCTS)} versucht man den erwarten Reward abhängig von einer gewählten Aktion $a$ zu schätzen. Genauer gesagt wird die $Q$-Funktion 
\begin{equation*}
	Q(s_0,a)=\E\set{\Delta}{s_0,a}
\end{equation*}
geschätzt, wobei der erwartete Reward durch ein Monte Carlo-Verfahren bestimmt wird. Das heißt man simuliert randomisiert viele \glqq Zukünfte\grqq\ und schätzt so den Reward abhängig von $a$. Das Simulieren einer Zukunft wird auch Rollout genannt.

Ein generisches MCTS-Verfahren sieht dann wie folgt aus.
\begin{lstlisting}
start tree V = {(*$v_0$*)}
while within budget do
	(*$v_l$*) := TreePolicy(V)
	append (*$v_l$*) to V
	(*$\Delta$*) := RolloutPolicy(V)
	Backup((*$v_l, \Delta$*))
end while
return best child of (*$v_0$*)
\end{lstlisting}

\begin{itemize}
	\item Mit der \textsc{TreePolicy}-Methode wird entschieden welcher Blattknoten expandiert wird. Der Rückgabewert ist also der vielversprechendste Knoten, abhängig von der Bewertungsfunktion.
	\item Die \textsc{RolloutPolicy} simuliert einen Rollout. Oft wird dies zufällig berechnet, mindestens jedoch randomisiert.
	\item \textsc{Backup} reicht den Reward vom Blattknoten zurück bis zur Wurzel durch. So kann im nächsten TreePolicy-Schritt von der Wurzel aus der beste Knoten ausgewählt werden. 
\end{itemize}
Man spricht von \emph{Flat Monte Carlo}, wenn die ausgerollten Zukünfte nicht gespeichert werden. Der Baum kann damit vergleichsweise flach gehalten werden. Entscheidend ist, dass das Baumwachstum - und damit der Speicheraufwand - mit diesem Verfahren auf vielversprechende Pfade fokussiert wird.

\paragraph{Upper Confidence Tree (UCT)}
Man kann auf das MCTS-Verfahren das bereits bekannte {\itshape Upper Confidence Bound}-System anwenden. Es handelt sich dabei dann um {\itshape Upper Confidence Tree (UCT)}.
\begin{itemize}
	\item Man wählt mithilfe von UCB den nächsten Knoten. Das heißt die \textsc{TreePolicy} wählt
	\begin{equation*}
		\underset{v'\in\partial(v)}\argmax\frac{Q(v')}{n(v')}+\beta\sqrt{\frac{2\ln n(v)}{n(v')}}
	\end{equation*}
	zum Expandieren.
	\item Die \textsc{Backup}-Methode aktualisiert alle Eltern $v$ von $v_l$ mit
	\begin{align*}
		n(v)&\leftarrow n(v)+1&&\text{Anzahl Rollouts}\\
		Q(v)&\leftarrow Q(v)+\Delta&&\text{Summe der Rewards.}
	\end{align*}
\end{itemize}


\subsection{Game Playing}
Die nachfolgenden Strategien sind für Spiele entworfen, in denen zwei Spieler gegeneinader antreten.


04/28ff
\paragraph{Minimax}
Die Idee des Minimax ist, den Pfad auszuwählen, der bei optimalem Opponenten noch den größten Gewinn verspricht.

Fassen wir den Gewinn des Spiels als eine Zahl (Reward) auf, lässt sich dieses Prinzip als rundenweises Minimieren bzw. Maximieren auffassen.
Der Spieler möchte den Reward maximieren, während das Ziel des Gegners ist, den Reward des Spielers zu minimieren. 

Bei einer Entscheidung des Spielers wird also der Pfad gewählt, der maximalen Gewinn verspricht. Simulieren wir nun Züge des Gegners, gehen wir von optimalem Handeln aus und lassen den Gegner den Reward minimieren. 

\begin{equation*}
	\underset a\argmax \mathrm{MinValue}()
\end{equation*}






Minimax findet einen optimalen Pfad, wenn der Baum endlich ist.
Allerdings wird ein optimaler Pfad nur dann gefunden wenn die Annahme, dass der Gegner optimal spielt stimmt. Ansonsten ist das Verhalten unklar und möglicherweise sehr schlecht.

Sei $b$ der branching factor, also die Zahl der möglichen Entscheidungen pro Zug (oder zumindest ein gutes Mittel) und $m$ die Zahl der Züge. Dann ist der Zeitaufwand die Größe des Baums, also $\mathcal O(b^m)$. Der Platzaufwand ist $\mathcal O(b*m)$, wie für eine Tiefensuche üblich.

\paragraph{$\alpha$-$\beta$ pruning}
Mit dem \emph{$\alpha$-$\beta$ pruning} können wir eine Verbesserung des Minimax-Verfahrens schaffen. 
Für viele Teilbäume kann schon sehr früh entschieden werden, dass sie keine guten Spielausgänge versprechen. Durch pruning des Baums kann der Zeitaufwand deutlich verbessert werden, dabei bleibt das Ergebnis trotzdem unverändert.



\paragraph{UCT for games}





\begin{itemize}
	\item Belief State
\end{itemize}



\input{MDPs.tex}
\input{RL.tex}
\input{CSPs.tex}








\section{Graphische Modelle von probabilistischen Systemen}
Unabhängigkeit von Zufallsvariablen sollte klar sein, wir haben wir diese Eigenschaft streng genommen sogar bereits benutzt, der Vollständigkeit halber definieren wir sie hier noch einmal.
\begin{definition}[Unabhängigkeit]
	Zwei Zufallsvariablen $X,Y$ heißen \emph{unabhängig}, wenn
	\begin{equation*}
		\independent(X,Y)\Longleftrightarrow P(X,Y)=P(X)*P(Y)
	\end{equation*}
	gilt. Analog sind sie \emph{konditional unabhängig}, wenn
	\begin{equation*}
		\independent(X,Y|Z)\Longleftrightarrow P(X,Y|Z)=P(X|Z)*P(Y|Z).
	\end{equation*}
\end{definition}








\subsection{Bayesnetze}
Wir gehen von drei Mengen an Zufallsvariablen aus, die beobachteten Variablen $Z$ und die Mengen $X,Y$ an versteckten Zufallsvariablen, wobei wir Inferenz über $X$, aber nicht $Y$ machen wollen.

Die Posterior-Randwahrscheinlichkeit für eine versteckte Variable $X$ ist
\begin{equation*}
	P(X|Z)=\frac{P(X,Z)}{P(Z)}=\frac{1}{P(Z)}\sum_{Y}P(X,Y,Z)
\end{equation*}
wobei alle $Y$ versteckt sind.




\begin{align*}
	P(R|H)&=\frac{1}{P(H)}\sum_SP(H|S,R)*P(S)\circ P(R)\\
	P(R|H=1)&\propto 
	\begin{pmatrix}
		0&0.9\\1&1
	\end{pmatrix}\begin{pmatrix}
		0.9\\0.1
	\end{pmatrix}\circ\begin{pmatrix}
		0.8\\0.2
	\end{pmatrix}
\end{align*}




\begin{definition}[Bayesnetz]
	Ein \emph{Bayesnetz} ist ein gerichteter azyklischer Graph, bei dem jeder Knoten eine Zufallsvariable repräsentiert. Für jeden Knoten $X_i$ gibt es eine konditionale Wahrscheinlichkeitsverteilung für dessen Wert
	\begin{equation*}
		P(X_i\,|\,\mathrm{Parents}(X_i)).
	\end{equation*}
	Üblicherweise stellt man beobachtete Variablen d.h. Variablen, deren Werte bekannt sind, mit grauer Schattierung dar.
\end{definition}
Die gemeinsame Verteilung kann man durch
\begin{equation*}
	P(X_{1:n})=\prod_{i=1}^nP(X_i\,|\,\mathrm{Parents}(X_i))
\end{equation*}
faktorisieren. Wobei $\mathrm{Parents}(X_i)$ die Vorgänger von $X_i$ sind, d.h. ein Knoten $v\in\mathrm{Parents}(X_i)$ genau dann, wenn eine Kante $(v,X_i)$ im Bayesnetz existiert.

Um Unabhängigkeit von Variablen in Bayesnetzen zu entscheiden gibt es grundsätzlich drei Fälle, die eine Hilfe sein können.
\begin{enumerate}
	\item Head to Head:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [below right of=X] 		{$Z$};
		  \node[state]         (Y) [above right of=Z] 		{$Y$};

		  \path (X) edge (Z)
		  (Y) edge (Z);
		\end{tikzpicture}
	\end{center}
	Es gilt $\independent(X,Y)$ und $\neg\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(X)P(Y)P(Z|X,Y)\\
		P(X,Y)&=P(X)P(Y)\sum_ZP(Z|X,Y)\\
		&=P(X)P(Y).
	\end{align*}

\item Tail to Tail:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [above right of=X] 		{$Z$};
		  \node[state]         (Y) [below right of=Z] 		{$Y$};

		  \path (Z) edge (X) edge (Y);
		\end{tikzpicture}
	\end{center}
	Es gilt $\neg\independent(X,Y)$ und $\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(Z)P(X|Z)P(Y|Z)\\
		P(X,Y|Z)&=\frac{P(X,Y,Z)}{P(Z)}\\
		&=P(X|Z)P(Y|Z).
	\end{align*}

\item Head to Tail:
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,
	                    semithick, baseline=1em]
		  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

		  \node[state] 		   (X)                    	{$X$};
		  \node[state]         (Z) [above right of=X] 		{$Z$};
		  \node[state]         (Y) [below right of=Z] 		{$Y$};

		  \path (X) edge (Z) 
		  (Z) edge (Y);
		\end{tikzpicture}
	\end{center}
	Es gilt $\neg\independent(X,Y)$ und $\independent(X,Y|Z)$, denn
	\begin{align*}
		P(X,Y,Z)&=P(X)P(Z|X)P(Y|Z)\\
		P(X,Y|Z)&=\frac{P(X,Y,Z)}{P(Z)}=\frac{P(X,Z)P(Y|Z)}{P(Z)}\\
		&=P(X|Z)P(Y|Z).
	\end{align*}

\end{enumerate}



\subsection{Inferenz in graphischen Modellen}
In Bayesnetzen können wir genauso Inferenz betreiben und aus Informationen der beobachteten Variablen oder a priori-Informationen Aussagen über andere Variablen treffen.

\subsubsection{Variablenelimination}
\begin{enumerate}
	\item Variablenelimination
	\item Faktorgraphen
	\item Message Passing
	\item Loopy belief propagation
	\item Sampling Methods?
\end{enumerate}

\subsection{Decision Making}
\subsection{Learning}
\subsection{Structure Learning}








\section{Dynamische Modelle}
\subsection{Markovprozesse bzw. -ketten}
Markovprozess erster Ordnung
\vspace{1em}
\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.3cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

	  \node[state] 		   (x-2)                    	{\scriptsize$X_{t-2}$};
	  \node[state]         (x-1) [right of=x-2] 		{\scriptsize$X_{t-1}$};
	  \node[state]         (x) [right of=x-1] 		{\scriptsize$X_{t}$};

	  \node[state]         (x1) [right of=x] 		{\scriptsize$X_{t+1}$};
	  \node[state]         (x2) [right of=x1]       {\scriptsize$X_{t+2}$};

	  \node[text width=0pt]			(start) [left=0.5cm of x-2] {};
	  \node[text width=0pt]			(end) [right=0.5cm of x2] {};

	  \path (start) edge (x-2)
	  (x-2) edge (x-1)
	  (x-1) edge (x)
	  (x) edge (x1)
	  (x1) edge (x2)
	  (x2) edge (end);
	\end{tikzpicture}
\end{center}
\vspace{1em}
Markovprozess zweiter Ordnung
\vspace{1em}
\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.3cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

	  \node[state] 		   (x-2)                    	{\scriptsize$X_{t-2}$};
	  \node[state]         (x-1) [right of=x-2] 		{\scriptsize$X_{t-1}$};
	  \node[state]         (x) [right of=x-1] 		{\scriptsize$X_{t}$};

	  \node[state]         (x1) [right of=x] 		{\scriptsize$X_{t+1}$};
	  \node[state]         (x2) [right of=x1]       {\scriptsize$X_{t+2}$};

	  \node[text width=0pt]			(start) [left=0.5cm of x-2] {};
	  \node[text width=0pt]			(end) [right=0.5cm of x2] {};

	  \path (start) edge (x-2) edge[in=135, out=45] (x-1)
	  (x-2) edge (x-1) edge[in=135, out=45] (x)
	  (x-1) edge (x) edge[in=135, out=45] (x1)
	  (x) edge (x1) edge[in=135, out=45] (x2)
	  (x1) edge (x2) edge[in=135, out=45] (end)
	  (x2) edge (end);
	\end{tikzpicture}
\end{center}
\vspace{1em}


\subsection{Versteckte Markovmodelle (HMM)}


\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.3cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.5cm, align=center]

	  \node[state] 		   (x0)                    	{\scriptsize$X_{0}$};
	  \node[state]         (x1) [right of=x0] 		{\scriptsize$X_{1}$};
	  \node[state]         (x2) [right of=x1] 		{\scriptsize$X_{2}$};
	  \node[]			(mid) [right of=x2] {$\cdots$};
	  \node[state]         (x4) [right of=mid]       {\scriptsize$X_{T}$};

	  \node[state, fill=observed] (y0) [below of=x0] {\scriptsize$Y_{0}$};
	  \node[state, fill=observed] (y1) [below of=x1] {\scriptsize$Y_{1}$};
	  \node[state, fill=observed] (y2) [below of=x2] {\scriptsize$Y_{2}$};
	  \node[state, fill=observed] (y4) [below of=x4] {\scriptsize$Y_{T}$};

	  \path (x0) edge (x1) edge (y0)
	  (x1) edge (x2) edge (y1)
	  (x2) edge (mid) edge (y2)
	  (mid) edge (x4)
	  (x4) edge (y4);
	\end{tikzpicture}
\end{center}
\vspace{1em}

\begin{equation*}
	P(X_{0:T},Y_{0:T})=P(X_0)\prod_{t=1}^TP(X_t|X_{t-1})\prod_{t=0}^TP(Y_t|X_t)
\end{equation*}

\subsection{Message Passing}


\noindent
\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    semithick, baseline=1em]
	  \tikzstyle{every state}=[fill=white,draw=black,text=black,text width=0.4cm, align=center]

	  \node[state] 		   (x)                    	{$X$};
	  \node[state]         (y) [right of=x] 		{$Y$};

	  \node[state, fill=observed] (a) [below=0.7cm of x] {$A$};
	  \node[state, fill=observed] (b) [below=0.7cm of y] {$B$};

	  \path (x) edge (y) edge (a)
	  (y) edge (b);
	\end{tikzpicture}
\end{center}
\vspace{1em}
Sei die die initiale Verteilung
\begin{equation*}
	P(X)=\matrix{0.5\\0.5},\\
\end{equation*}
sowie die Übergangs- bzw. Sensormatrizen
\begin{equation*}
	P(A|X)=P(Y|X)=P(B|Y)=\matrix{0.8&0.2\\0.2&0.8}
\end{equation*}
gegeben. Wir können eine Aussage über $Y$ gegeben einer Beobachtung der $A,B$ treffen
\begin{equation*}
	P(Y|A=0,B=1)=\mu_{X\rightarrow Y}\circ\mu_{B\rightarrow Y}.
\end{equation*}
Die $\mu$ sind \emph{Messages}. Diese berechnen sich allgemein als
\begin{equation*}
	\mu_{X_i\rightarrow x_j}=P(X_j|X_i)*P(X_i).
\end{equation*}
Wir erhalten also zunächst
\begin{align*}
	\mu_{B\rightarrow Y}&=P(Y|B)*P(B)\propto P(B|Y)^T*\vector{0\\1}=\vector{0.2\\0.8}\\
	\mu_{A\rightarrow X}&=P(X|A)*P(A)\propto P(A|X)^T*\vector{1\\0}=\vector{0.8\\0.2}.
\end{align*}
Womit wir zunächst $P(X|A=0)$ und die dritte Message berechnen können
\begin{align*}
	\mu_{X\rightarrow Y}&=P(Y|X)*P(X|A=0)\\
	&\propto P(Y|X)*(\mu_{A\rightarrow X}\circ P(X))\\
	&=\matrix{0.8&0.2\\0.2&0.8}*\vector{0.4\\0.1}\\
	&=\vector{0.8*0.4+0.2*0.1\\0.2*0.4+0.8*0.1}=\vector{0.34\\0.16}.
\end{align*}
Insgesamt erhalten wir
\begin{align*}
	P(Y|A=0,B=1)&\propto\mu_{X\rightarrow Y}\circ\mu_{B\rightarrow Y}\\
	&=\vector{0.34\\0.16}\circ \vector{0.2\\0.8}=\vector{0.068\\0.128}.
\end{align*}



\section{Neuronale Netze}
\subsection{Graphische Repräsentationen}


\section{Erklärbarkeit von Entscheidungen}







\section{unassigned}
\begin{itemize}
	\item Belief Space Planning for Bandits 07/22ff
\end{itemize}
\end{document}
